{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w2GM8vtc_FBB",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# LangChain Basic\n",
        "**2024 삼성전자 무선사업부 생성형 AI 교육**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dzXcYq79_FBE",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "1. 패키지 설치\n",
        "2. 기본 패키지 & 환경 설정\n",
        "3. LLM\n",
        "4. Embedding\n",
        "5. Prompt Template\n",
        "6. Fewshot Prompt Template\n",
        "7. Chain + Memory "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": false,
        "gather": {
          "logged": 1700910339407
        },
        "id": "9EbvSmJl_FBE",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "8e46b7f4-2875-4286-8a0d-4c78481249b9"
      },
      "outputs": [],
      "source": [
        "%pip install tiktoken\n",
        "%pip install langchain\n",
        "%pip install -U langchain-openai\n",
        "%pip install langchain_community\n",
        "%pip install langchain_core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1716690827922
        },
        "id": "Ua4tsQUd_FBG",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# 2. 기본 패키지 설정 & AzureOpenAI 환경설정\n",
        "\n",
        "# 환경 변수 설정!! \n",
        "import os\n",
        "import langchain\n",
        "\n",
        "from config_azure import (\n",
        "    AZURE_OPENAI_API_VERSION,\n",
        "    AZURE_OPENAI_ENDPOINT,\n",
        "    AZURE_OPENAI_KEY\n",
        ")\n",
        "# lagchain 사용시 반드시 환경변수 값 사용!!\n",
        "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_OPENAI_KEY\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_OPENAI_ENDPOINT\n",
        "os.environ[\"OPENAI_API_VERSION\"] = AZURE_OPENAI_API_VERSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1716690831513
        },
        "id": "_1tw9gbs6BX1",
        "outputId": "b15d2181-dd2f-42b0-f602-eff80e906552"
      },
      "outputs": [],
      "source": [
        "# 3-1. OpenAI LLM(CompletionModel 연결)\n",
        "\n",
        "from langchain_openai import AzureOpenAI\n",
        "\n",
        "com_llm = AzureOpenAI(\n",
        "    azure_deployment=\"gpt-35-turbo-instruct\",\n",
        "    max_tokens=128,\n",
        "    temperature=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716690832088
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "com_llm.invoke(\"오늘 저녁 메뉴는\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1716690832452
        },
        "id": "VMWlGkXF_FBH",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# 3-2. OpenAI LLM(Chat Model 연결)\n",
        "\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "\n",
        "chat_llm = AzureChatOpenAI(\n",
        "    deployment_name=\"gpt-35-turbo\",\n",
        "    max_tokens=256,\n",
        "    temperature=0.9,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716690835185
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "chat_llm.invoke(\"오늘 저녁 메뉴는\").content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1716690838431
        },
        "id": "12PLolMg_FBH",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# 3-3. Chat llm 사용\n",
        "\n",
        "from langchain.schema import (\n",
        "    SystemMessage, # system\n",
        "    AIMessage, # assistant\n",
        "    HumanMessage # user\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"RPG 게임 캐릭터 재키, 성을 지키는 용감한 용사로 유저의 질문에 반말 구어체로 짧게 답한다\"),\n",
        "    HumanMessage(content=\"안녕?\"),\n",
        "    AIMessage(content=\"용감한 자여, 전설의 성에 온걸 환영한다!\"),\n",
        "    HumanMessage(content=\"너의 이름은 뭐니?\"),\n",
        "    AIMessage(content=\"사람들은 나를 재키라 부르지. 성을 지키는 임무를 맡았으니 무엇이든 나에게 물어보라.\"),\n",
        "    HumanMessage(content=\"용을 만나려면 어떻게해야하지?\"),\n",
        "]\n",
        "\n",
        "chat_llm.invoke(messages).content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716690841160
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# 4-1. Embedding 모델 연결\n",
        "\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "\n",
        "embedding_model = AzureOpenAIEmbeddings(\n",
        "    azure_deployment=\"text-embedding-ada-002\",\n",
        ")\n",
        "\n",
        "embedding_model.embed_query(\"안녕하세요\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716605172168
        },
        "id": "SbZWItYx_FBH"
      },
      "outputs": [],
      "source": [
        "# 4-2. 유사도\n",
        "\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cos_sim(word1, word2):\n",
        "    em1 = embedding_model.embed_query(word1)\n",
        "    em2 = embedding_model.embed_query(word2)\n",
        "    return dot(em1, em2)/(norm(em1)*norm(em2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": false,
        "gather": {
          "logged": 1716690859926
        },
        "id": "cUQyT2wV_FBI",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "fad3b31e-d707-4878-b1e5-957758cbc945"
      },
      "outputs": [],
      "source": [
        "# 5. Chat Prompt Template(Chat Model Prompt)\n",
        "\n",
        "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
        "\n",
        "system_template = \"\"\"\n",
        "한국어 문장을 {output_language}로 번역하는 인공지능.\n",
        "번역된 문장의 한글 발음을 괄호 안에 함께 제공한다.\n",
        "예) Hi(하이), こんにちは(곤니치와)\n",
        "\"\"\"\n",
        "\n",
        "system_message_prompt_template = SystemMessagePromptTemplate.from_template(\n",
        "    system_template)\n",
        "\n",
        "human_template = \"{input_text}\"\n",
        "\n",
        "human_message_prompt_template = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [system_message_prompt_template, human_message_prompt_template])\n",
        "\n",
        "final_prompt = chat_prompt_template.format_prompt(output_language=\"영어\",\n",
        "                          input_text=\"오늘 날씨 정말 좋네요.\").to_messages()\n",
        "\n",
        "print(final_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716605172595
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "chat_llm.invoke(final_prompt).content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "gather": {
          "logged": 1716690861508
        },
        "id": "HlOmdpRwNgYt",
        "outputId": "7f7f0401-62e6-4878-dea8-60a5f0734204"
      },
      "outputs": [],
      "source": [
        "# 6. PromptTemplate ⭐️⭐️⭐️\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "한국어 문장 {input_text}을 {output_language}로 번역하는 인공지능.\n",
        "번역된 문장의 한글 발음을 괄호 안에 함께 제공한다.\n",
        "예) Hi(하이), こんにちは(곤니치와)\n",
        "\"\"\"\n",
        "\n",
        "translate_template = PromptTemplate(\n",
        "    input_variables=[\"input_text\", \"output_language\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "final_prompt = translate_template.format_prompt(input_text=\"오늘 날씨가 정말 좋아요\", \n",
        "                                                output_language=\"일본어\").to_messages()\n",
        "print(final_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716605173015
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "chat_llm.invoke(final_prompt).content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1716690863898
        },
        "id": "9epEKSyQFEZQ",
        "outputId": "237be70c-4806-4a67-8726-d47ef99a03d7"
      },
      "outputs": [],
      "source": [
        "# 6-2. Few shot learning\n",
        "\n",
        "from langchain.prompts import FewShotPromptTemplate\n",
        "# fewshot 예시 문장\n",
        "examples = [\n",
        "    {\"input\": \"영어 : 이 와인 정말 맛있네요\", \"output\": \"This wine is really delicious.(발음:디스 와인 이즈 리얼리 딜리셔스)\"},\n",
        "    {\"input\": \"프랑스어 : 이 와인 정말 맛있네요\", \"output\": \"Ce vin est vraiment délicieux.(발음:세 반 에트랑제 드리슈)\"},\n",
        "    {\"input\": \"독일어 : 이 와인 정말 맛있네요 \", \"output\": \"Dieser Wein schmeckt wirklich lecker.(발음:디저 바인 샴케 비역클러)\"},\n",
        "    {\"input\": \"일본어 : 이 와인 정말 맛있네요\", \"output\": \"このワインは本当においしいですね。(발음:코노 와인와 혼토니 오이시이 데스 네)\"},\n",
        "    {\"input\": \"이태리어 : 이 와인 정말 맛있네요\", \"output\": \"Questo vino è davvero delizioso.(발음:쿼스토 비노 에 디버로 델리조오소)\"},\n",
        "    {\"input\": \"이태리어 : 화장실이 어디인가요?\", \"output\": \"Dov'è il bagno?(발음:도베 이르 바뇨?\"},\n",
        "    {\"input\": \"일본어 : 화장실이 어디인가요?\", \"output\": \"トイレはどこですか？(발음:토이레와 도코데스카?)\"},\n",
        "    {\"input\": \"독일어 : 화장실이 어디인가요?\", \"output\": \"Entschuldigung, wo ist die Toilette?(발음:앤쇨룰디귄, 보이스 디 토일레트?)\"},\n",
        "    {\"input\": \"프랑스어 : 화장실이 어디인가요?\", \"output\": \"Où sont les toilettes?(발음:우 솽 레 투알레트?)\"},\n",
        "    {\"input\": \"영어 : 화장실이 어디인가요?\", \"output\": \"Where is the restroom?(발음:웨어 이즈 더 레스트룸?)\"},\n",
        "]\n",
        "\n",
        "# fewshot 예시 문장+템플릿 결합\n",
        "example_prompt = PromptTemplate(input_variables=[\"input\", \"output\"],\n",
        "                                template=\"문장을 주어진 언어로 번역한다. 번역된 문장의 한국어 발음을 괄호 안에 함께 표기한다. : {input} -> {output}\")\n",
        "\n",
        "fewshot_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    suffix=\"문장을 주어진 언어로 번역하고 한글 발음을 괄호안에 표기한다.  : {input} -> \",\n",
        "    input_variables=[\"input\"]\n",
        ")\n",
        "\n",
        "final_prompt=fewshot_prompt.format_prompt(input=\"일본어 : 날씨가 너무 더워요\").to_messages()\n",
        "print(final_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716605173458
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "chat_llm.invoke(final_prompt).content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1716690865637
        },
        "id": "LPSmQ8GE_FBJ",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# 7-1. Chain-PromptTemplate 연결\n",
        "# LangChain without LCEL:\n",
        "\n",
        "from langchain.chains import LLMChain \n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# 체인 = 모델 + 프롬프트 + Parser\n",
        "chain = LLMChain(llm=chat_llm, \n",
        "                 prompt=fewshot_prompt,\n",
        "                 output_parser=output_parser)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### LangChain Expression Language (LCEL)\n",
        "* [LCEL 공식 문서 바로가기](https://python.langchain.com/v0.1/docs/expression_language/)\n",
        "* [LCEL Parsers 공식 문서](https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716605173630
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# 7-2. LCEL(LangChain Express Language) 활용 코드\n",
        "# (model(prompt)) -> output_parser(model(prompt))\n",
        "\n",
        "chain = fewshot_prompt | chat_llm | output_parser\n",
        "com_chain = fewshot_prompt | com_llm | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716690869706
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# 7-3. Batch : 여러개 인풋 활용\n",
        "\n",
        "input_list = [\n",
        "    {\"input\" : \"영어 : 이 와인 정말 맛있네요\"},\n",
        "    {\"input\" : \"이태리어 : 이 와인 정말 맛있네요\"},\n",
        "    {\"input\" : \"프랑스어 : 이 와인 정말 맛있네요\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Simple Sequential Chain\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qrct9oGPZklQSWwlPNKfqQ.png)\n",
        "[Image Reference](https://faun.pub/langchain-in-chains-11-chains-33b9f3c2d217)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716690932254
        },
        "id": "RQoOOaeaKAxJ"
      },
      "outputs": [],
      "source": [
        "# 8-1. 두 개 체인 연결 \n",
        "# without LCEL\n",
        "\n",
        "# SimpleSequentialChain: 하나의 input variable 만 사용가능\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "# 첫번째 체인(한국어->외국어)\n",
        "fisrt_chain = LLMChain(llm = chat_llm,\n",
        "                 prompt = fewshot_prompt)\n",
        "\n",
        "# 두번째 체인\n",
        "second_prompt = PromptTemplate(\n",
        "    input_variables=[\"foreign_language\"],\n",
        "    template=\"번역된 문장과 동일한 뜻의 유사 표현 5개와 번역된 문장의 한국어 발음 표기를 괄호 안에 표기한다: {foreign_language}\",\n",
        ")\n",
        "\n",
        "second_chain = LLMChain(llm=chat_llm, prompt=second_prompt)\n",
        "\n",
        "# 두가지 체인 결합\n",
        "overall_chain = SimpleSequentialChain(chains=[fisrt_chain, second_chain], verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716690952493
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# 8-2. With LCEL\n",
        "\n",
        "chain1 = fewshot_prompt | chat_llm \n",
        "chain2 = second_prompt | chat_llm \n",
        "\n",
        "overall_chain = chain1 | chain2 | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716690933458
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# 줄바꿈 custom parser\n",
        "def line_parse(output) -> str:\n",
        "    lines = output.split('\\n')\n",
        "    return lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### SequentialChain\n",
        "![]()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716690974019
        },
        "id": "CmGDWdGvL43S"
      },
      "outputs": [],
      "source": [
        "# 9-1. 여러개 인풋 사용 가능한 Sequential Chain \n",
        "# without LCEL\n",
        "\n",
        "from langchain.chains import SequentialChain\n",
        "\n",
        "# 첫번째 체인\n",
        "menu_template = \"\"\"\n",
        "신메뉴 이름 {name}이 제공됩니다.\n",
        "주재료는 {ingredients}입니다.\n",
        "주어진 정보를 바탕으로 배달 서비스에 쓰일 신메뉴 소개글을 100자로 작성하시오\n",
        "\"\"\"\n",
        "\n",
        "menu_prompt = PromptTemplate(\n",
        "    input_variables=['name', 'ingredients'],\n",
        "    template=menu_template\n",
        ")\n",
        "\n",
        "menu_chain = LLMChain(llm=com_llm,\n",
        "                 prompt=menu_prompt,\n",
        "                 output_key=\"description\")\n",
        "\n",
        "# 두번째 체인\n",
        "review_template = \"\"\"\n",
        "다음 신메뉴를 배달 주문으로 먹은 손님의 긍정 한줄 리뷰 3개 작성 \n",
        "{description}\n",
        "\"\"\"\n",
        "\n",
        "review_prompt = PromptTemplate(\n",
        "    input_variables=[\"description\"],\n",
        "    template=review_template\n",
        ")\n",
        "\n",
        "review_chain = LLMChain(llm=chat_llm,\n",
        "                     prompt=review_prompt,\n",
        "                     output_key=\"review\")\n",
        "\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[menu_chain, review_chain],\n",
        "    input_variables=[\"name\", \"ingredients\"],\n",
        "    output_variables=[\"description\", \"review\"],\n",
        "    verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716691000207
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "overall_chain.invoke({\"name\":\"\", \"ingredients\": \"\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716690992013
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# 9-2. SequentialChain with LCEL\n",
        "\n",
        "# 첫번째 체인\n",
        "menu_template = \"\"\"\n",
        "신메뉴 이름 {name}이 제공됩니다.\n",
        "주재료는 {ingredients}입니다.\n",
        "주어진 정보를 바탕으로 배달 서비스에 쓰일 신메뉴 소개글을 100자로 작성하시오\n",
        "\"\"\"\n",
        "\n",
        "menu_prompt = PromptTemplate(\n",
        "    input_variables=['name', 'ingredients'],\n",
        "    template=menu_template\n",
        ")\n",
        "\n",
        "\n",
        "# 두번째 체인\n",
        "review_template = \"\"\"\n",
        "다음 신메뉴를 배달 주문으로 먹은 손님의 긍정 한줄 리뷰 3개 작성 \n",
        "{description}\n",
        "\"\"\"\n",
        "\n",
        "review_prompt = PromptTemplate(\n",
        "    input_variables=[\"description\"],\n",
        "    template=review_template\n",
        ")\n",
        "\n",
        "chain1 = menu_prompt | chat_llm\n",
        "chain2 = review_prompt | chat_llm \n",
        "\n",
        "overall_chain = {\"description\" : chain1 } | chain2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 중간 소개글 출력 함수\n",
        "def run_chain_with_print(output) -> dict:\n",
        "    print(\"소개글:\", output.content)\n",
        "    print(\"리뷰:\")\n",
        "    return {\"description\" : output}\n",
        "\n",
        "overall_chain = chain1 | run_chain_with_print | chain2 | output_parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### LangChain Memory\n",
        "![](https://python.langchain.com/v0.1/assets/images/memory_diagram-0627c68230aa438f9b5419064d63cbbc.png)\n",
        "\n",
        "* [LangChain Memory 공식 문서](https://python.langchain.com/v0.1/docs/modules/memory/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716695799351
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# 10-1. LangChain Memory\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# 메모리 생성\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "# 메모리 값 저장1\n",
        "memory.chat_memory.add_user_message(\"안녕, 내 이름은 xx야!\")\n",
        "memory.chat_memory.add_ai_message(\"안녕, 내 이름은 yy야\")\n",
        "\n",
        "# 메모리 값 저장2\n",
        "memory.save_context(\n",
        "    {\"input\" : \"안녕, 내 이름은 aa야\"},\n",
        "    {\"output\" : \"반가워, 내이름은 bb야\"}\n",
        ")\n",
        "\n",
        "# 저장된 메모리 출력\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716696032253
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# 10-2. Chain-Memory 연결 without LCEL\n",
        "\n",
        "memory_template = \"\"\"사용자의 질문에 유용한 정보를 알려주는 챗봇 도우미\n",
        "\n",
        "이전 대화 기록 : {chat_history}\n",
        "사용자 질문 : {question}\n",
        "답변 : \n",
        "\"\"\"\n",
        "\n",
        "# prompte template 생성\n",
        "memory_prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"question\"],\n",
        "    template=memory_template\n",
        ")\n",
        "# memory_prompt = PromptTemplate.from_template(memory_template)\n",
        "\n",
        "# 메모리 생성\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "# Chain 생성\n",
        "memory_chain = LLMChain(llm=com_llm,\n",
        "                        prompt=memory_prompt,\n",
        "                        verbose=True,\n",
        "                        memory=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716696751201
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# 10-3. Chain-Memory 연결2 without LCEL : ChatPrompt\n",
        "\n",
        "from langchain_core.prompts import (\n",
        "    MessagesPlaceholder,\n",
        ")\n",
        "\n",
        "memory_prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        SystemMessage(content=\"RPG 게임 캐릭터 재키, 성을 지키는 용감한 용사로 유저의 질문에 반말 구어체로 짧게 답한다\"),\n",
        "        HumanMessage(content=\"안녕?\"),\n",
        "        AIMessage(content=\"용감한 자여, 전설의 성에 온걸 환영한다!\"),\n",
        "        HumanMessage(content=\"너의 이름은 뭐니?\"),\n",
        "        AIMessage(content=\"사람들은 나를 재키라 부르지. 성을 지키는 임무를 맡았으니 무엇이든 나에게 물어보라.\"),\n",
        "\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "chat_chain = LLMChain(llm=chat_llm,\n",
        "                      prompt=memory_prompt,\n",
        "                      verbose=True,\n",
        "                      memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716697032873
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# 10-4. Memory Chain with LCEL\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# 메모리 초기화\n",
        "memory.clear()\n",
        "\n",
        "# 메모리 불러오기 함수(사용하지 않는 인자 _ 생성)\n",
        "def load_memory(_):\n",
        "    return memory.load_memory_variables({})[\"chat_history\"]\n",
        "\n",
        "\n",
        "# RunnablePassthrough.assign : chat_history라는 변수에 함수 실행 결과값 할당\n",
        "lcel_chain = RunnablePassthrough.assign(chat_history=load_memory) | memory_prompt | chat_llm | output_parser\n",
        "\n",
        "def invoke_lcel_chain(question):\n",
        "    result = lcel_chain.invoke({\"question\": question})\n",
        "    memory.save_context(\n",
        "        {\"input\": question},\n",
        "        {\"output\": result},\n",
        "    )\n",
        "    print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716697035602
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# 10-5. lcel 질문\n",
        "invoke_lcel_chain(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 대화 기록 \n",
        "load_memory(_)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
